# åŸºäºæ··åˆç­–ç•¥ä¸æ–‡æœ¬èåˆçš„å¢å¼ºå‹ SASRec æ¨èç³»ç»Ÿ

æœ¬ä»“åº“æ˜¯ [WEBæœç´¢ä¸æ¨èç³»ç»Ÿå¯¼è®º] è¯¾ç¨‹è®¾è®¡çš„ä»£ç å®ç°ã€‚æœ¬é¡¹ç›®é’ˆå¯¹ä¼ ç»Ÿ SASRec æ¨¡å‹å­˜åœ¨çš„**å†·å¯åŠ¨/çŸ­åºåˆ—æ¨èéš¾**ã€**ä¾§é¢ä¿¡æ¯åˆ©ç”¨ä¸è¶³**ä»¥åŠ**é²æ£’æ€§æ¬ ä½³**ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

1.  **å¤šæ¨¡æ€ç‰¹å¾èåˆ (Text Fusion)**:
    - å¼•å…¥ç‰©å“çš„æ–‡æœ¬ Embeddingï¼ˆå¦‚ BERT æå–ï¼‰ã€‚
    - é€šè¿‡ Deep Fusion Layer å°†è¯­ä¹‰ä¿¡æ¯ä¸ ID Embedding æ·±åº¦èåˆã€‚
2.  **å¯¹æ¯”å­¦ä¹ å¢å¼º (Contrastive Learning)**:
    - å¼•å…¥ InfoNCE Loss ä½œä¸ºè¾…åŠ©ä»»åŠ¡ã€‚
    - ä½¿ç”¨åºåˆ—å¢å¼ºï¼ˆMask/Cropï¼‰æœ€å¤§åŒ–åŒä¸€åºåˆ—ä¸åŒè§†å›¾é—´çš„äº’ä¿¡æ¯ï¼Œæå‡é²æ£’æ€§ã€‚
3.  **é•¿çŸ­åºåˆ—æ··åˆæ¨ç† (Hybrid Strategy)**:
    - **çŸ­åºåˆ— (< Threshold)**: è‡ªåŠ¨åˆ‡æ¢ä½¿ç”¨ N-Gram (1-Gram/2-Gram) ç»Ÿè®¡æ¨¡å‹ï¼Œæ•æ‰å¼ºå…³è”è§„åˆ™ã€‚
    - **é•¿åºåˆ— (>= Threshold)**: ä½¿ç”¨å¢å¼ºå‹ SASRec æ¨¡å‹ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

- `model.py`: **æ ¸å¿ƒæ¨¡å‹**ã€‚åŒ…å« `SASRec` ç±»ï¼Œé›†æˆäº† Text Fusion æ¨¡å—å’Œ Contrastive Learning Loss è®¡ç®—ã€‚
- `train.py`: **è®­ç»ƒè„šæœ¬**ã€‚æ”¯æŒæ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–ã€è”åˆ Loss ä¼˜åŒ– (`Rec_Loss + lambda * CL_Loss`)ã€‚
- `evaluate_model.py`: **è¯„ä¼°è„šæœ¬**ã€‚å®ç°äº† Hybrid Inference é€»è¾‘ï¼Œæ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€åˆ‡æ¢ N-Gram ä¸ SASRecã€‚
- `analyze_data.py`: **æ•°æ®åˆ†æ**ã€‚ç”Ÿæˆæ•°æ®åˆ†å¸ƒå›¾ï¼ˆåºåˆ—é•¿åº¦ã€ç‰©å“æµè¡Œåº¦ç­‰ï¼‰ã€‚
- `build_ngram.py`: æ„å»º N-Gram ç»Ÿè®¡æ¨¡å‹ã€‚
- `extract_text_features.py`: æå–ç‰©å“æ–‡æœ¬ç‰¹å¾ï¼ˆé¢„å¤„ç†ï¼‰ã€‚
- `dataset.py`: PyTorch Dataset å®šä¹‰ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
pip install torch pandas numpy scikit-learn tqdm matplotlib
```

### 2. æ•°æ®å‡†å¤‡
è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶ï¼ˆå¦‚ `train_augmented.csv`, `test2.csv`ï¼‰å’Œé¢„è®­ç»ƒ Embedding (`item_embeddings.pkl`) ä½äºé¡¹ç›®æ ¹ç›®å½•ã€‚
*(æ³¨ï¼šç”±äºæ•°æ®æ–‡ä»¶è¾ƒå¤§ï¼ŒæœªåŒ…å«åœ¨ git ä»“åº“ä¸­)*

### 3. è®­ç»ƒæ¨¡å‹
```bash
python train.py
```
è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åœ¨ `train_log_enhanced.txt`ã€‚

### 4. è¯„ä¼°æ¨¡å‹
```bash
python evaluate_model.py
```
è¯¥è„šæœ¬å°†åŠ è½½æœ€ä½³æ¨¡å‹ `sasrec_best.pth` å’Œ N-Gram æ¨¡å‹ `ngram_model_enhanced.pkl` è¿›è¡Œæ··åˆæ¨ç†è¯„ä¼°ã€‚

### 5. æ•°æ®åˆ†æ
```bash
python analyze_data.py
```
ç”Ÿæˆçš„åˆ†å¸ƒå›¾å°†ä¿å­˜åœ¨ `analysis_output/` ç›®å½•ã€‚

## ğŸ“Š å®éªŒç»“æœ

| æ¨¡å‹é…ç½® | MRR@10 | ç›¸å¯¹æå‡ (vs Baseline) |
| :--- | :---: | :---: |
| Standard SASRec | 0.0636 | - |
| SASRec + Text Fusion | 0.0662 | +4.1% |
| SASRec + CL | 0.0641 | +0.8% |
| **Hybrid (Ours)** | **0.0673** | **+5.8%** |

## ğŸ”— å‚è€ƒ

- Kang, W. C., & McAuley, J. (2018). Self-Attentive Sequential Recommendation. ICDM.
- Xie, X., et al. (2022). Contrastive Learning for Sequential Recommendation. ICDE.
